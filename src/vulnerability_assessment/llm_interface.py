import logging
import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig
import re # For more robust JSON extraction
import os

logger = logging.getLogger(__name__)

# Global variables to hold the loaded model and tokenizer
# This avoids reloading the model for every packet/query
llm_model = None
llm_tokenizer = None
llm_device = None
MODEL_NAME = "openlm-research/open_llama_3b_v2" # Define the model

def load_llm_model(model_name_or_path: str = MODEL_NAME):
    """Loads the specified LLM model and tokenizer."""
    global llm_model, llm_tokenizer, llm_device

    if llm_model is not None and llm_tokenizer is not None:
        logger.debug("LLM model and tokenizer already loaded.")
        return True

    try:
        logger.info(f"Loading LLM model: {model_name_or_path}...")

        # Determine device (GPU if available, otherwise CPU)
        if torch.cuda.is_available():
            llm_device = torch.device("cuda")
            logger.info("CUDA (GPU) is available. Loading model on GPU.")
            # For larger models, consider device_map="auto" with accelerate
            # For 3B, loading directly to GPU might be fine, or use float16
            model_kwargs = {"torch_dtype": torch.float16, "device_map": "auto"} # Requires accelerate
        else:
            llm_device = torch.device("cpu")
            logger.info("CUDA not available. Loading model on CPU.")
            # Float16 might not be optimal on CPU, stick to default precision or bfloat16 if supported
            model_kwargs = {} # Let transformers handle default dtype for CPU

        llm_tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)
        llm_model = AutoModelForCausalLM.from_pretrained(model_name_or_path, **model_kwargs)

        # Some models might require explicit setting of pad_token_id if not set
        if llm_tokenizer.pad_token_id is None:
             # If there's no pad token, use eos token for padding during generation
             logger.warning("Tokenizer does not have a pad token set. Using EOS token ID for padding.")
             llm_tokenizer.pad_token_id = llm_tokenizer.eos_token_id
             llm_model.config.pad_token_id = llm_model.config.eos_token_id


        # Set model to evaluation mode
        llm_model.eval()

        logger.info(f"LLM model '{model_name_or_path}' loaded successfully on {llm_device}.")
        return True

    except Exception as e:
        logger.error(f"Failed to load LLM model '{model_name_or_path}': {e}", exc_info=True)
        llm_model = None
        llm_tokenizer = None
        llm_device = None
        return False

def create_llm_prompt(features: dict) -> str:
    """Creates a structured prompt for the Llama model based on packet features."""
    # Instruction-based prompt suitable for models like Llama
    prompt = f"""[INST] You are a network security analyst. Analyze the following network packet features for potential vulnerabilities. Respond ONLY with a valid JSON object containing your analysis.

Packet Features:
- Timestamp: {features.get('timestamp', 'N/A')}
- Source IP: {features.get('src_ip', 'N/A')}
- Destination IP: {features.get('dst_ip', 'N/A')}
- Protocol: {features.get('protocol', 'N/A')}"""

    if 'src_port' in features:
        prompt += f"\n- Source Port: {features.get('src_port')}"
    if 'dst_port' in features:
        prompt += f"\n- Destination Port: {features.get('dst_port')}"
    if 'tcp_flags' in features:
        prompt += f"\n- TCP Flags: {features.get('tcp_flags')}"
    if 'icmp_type' in features:
        prompt += f"\n- ICMP Type: {features.get('icmp_type')}, Code: {features.get('icmp_code', 'N/A')}"

    prompt += f"\n- Packet Length: {features.get('length', 'N/A')}"
    payload = features.get('payload_snippet', '')
    if payload:
        # Escape potential JSON breaking characters in payload for the prompt display if needed,
        # but model should handle it. Keep it clean for analysis. Limit length.
        safe_payload = payload.replace('\n', ' ').replace('\r', '')[:100] # Show limited, cleaned snippet
        prompt += f"\n- Payload Snippet (first {len(safe_payload)} chars): {safe_payload}"
    else:
        prompt += "\n- Payload Snippet: [None]"

    prompt += """

Analysis Task:
Identify potential vulnerabilities (e.g., Exploit Attempt, Malware C&C, Suspicious Scan, Protocol Anomaly).
If a vulnerability is detected:
1. Set "is_vulnerable" to true.
2. Provide a concise "vulnerability_type".
3. Assign a "severity" (Critical, High, Medium, Low, Informational).
4. Write a brief "description" explaining the finding.
5. Suggest actionable enterprise "mitigation_steps" as a list of strings.
If no vulnerability is detected, set "is_vulnerable" to false and omit other keys.

Respond ONLY with the JSON object. Example format:
{
  "is_vulnerable": true,
  "vulnerability_type": "Example Vulnerability",
  "severity": "High",
  "description": "Explanation of the detected vulnerability based on features.",
  "mitigation_steps": ["Step 1...", "Step 2..."]
}
OR
{
  "is_vulnerable": false
}

JSON Output:[/INST]
"""
    # The [/INST] signifies the end of the instruction for Llama models
    return prompt

def extract_json_output(raw_output: str, prompt: str) -> dict | None:
    """Attempts to extract the JSON part from the LLM's raw output."""
    logger.debug(f"Attempting to extract JSON from raw output.")
    # Remove the prompt part from the output to isolate the generation
    generation_part = raw_output.replace(prompt, "").strip()
    logger.debug(f"Generation part: {generation_part}")

    # Try to find the JSON block using regex, looking for balanced braces
    # This is more robust than simple start/end searches but still not foolproof
    match = re.search(r"\{.*\}", generation_part, re.DOTALL)
    if match:
        json_string = match.group(0)
        logger.debug(f"Potential JSON string found: {json_string}")
        try:
            parsed_json = json.loads(json_string)
            # Basic validation: check for expected keys if vulnerable
            if parsed_json.get("is_vulnerable") is True:
                 if not all(k in parsed_json for k in ["vulnerability_type", "severity", "description", "mitigation_steps"]):
                      logger.warning(f"Parsed JSON is marked vulnerable but missing required keys: {parsed_json}")
                      return {"is_vulnerable": False, "error": "Incomplete JSON structure from LLM"}
            elif parsed_json.get("is_vulnerable") is False:
                 # Ensure no extra keys are present if not vulnerable
                 if len(parsed_json) > 1:
                     logger.warning(f"Parsed JSON is marked not vulnerable but contains extra keys: {parsed_json}")
                     return {"is_vulnerable": False} # Return the clean version
            else:
                 # 'is_vulnerable' key missing or not boolean
                 logger.warning(f"Parsed JSON missing or has invalid 'is_vulnerable' key: {parsed_json}")
                 return {"is_vulnerable": False, "error": "Invalid 'is_vulnerable' key in JSON from LLM"}

            logger.debug("JSON parsed successfully.")
            return parsed_json
        except json.JSONDecodeError as e:
            logger.error(f"Failed to decode JSON from extracted string: {e}\nString was: {json_string}", exc_info=False)
            return {"is_vulnerable": False, "error": f"JSON parsing failed: {e}"}
    else:
        logger.error(f"Could not find JSON block in LLM generation: {generation_part}")
        return {"is_vulnerable": False, "error": "No JSON object found in LLM response"}


def query_llm(features: dict, model_name_or_path: str = MODEL_NAME) -> dict:
    """
    Queries the loaded fine-tuned LLM with packet features using Hugging Face Transformers.

    Args:
        features: Dictionary of features from feature_extractor.
        model_name_or_path: Identifier for the model to use (passed to load_llm_model).

    Returns:
        A dictionary containing the LLM's analysis results (parsed from JSON).
    """
    # Ensure model is loaded
    if not load_llm_model(model_name_or_path):
         logger.error("LLM model is not available. Cannot perform assessment.")
         # Return a structured error
         return {"is_vulnerable": False, "error": "LLM model not loaded"}

    prompt = create_llm_prompt(features)
    logger.debug(f"LLM Prompt:\n{prompt[:500]}...") # Log truncated prompt

    analysis_result = {"is_vulnerable": False, "error": "Inference did not run"} # Default error state

    try:
        logger.debug("Tokenizing prompt...")
        inputs = llm_tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1024).to(llm_device) # Adjust max_length if needed

        # Generation configuration
        # Use lower temperature for more deterministic JSON output
        generation_config = GenerationConfig(
            max_new_tokens=300,  # Max tokens for the JSON output
            temperature=0.1,     # Low temperature for factuality/format adherence
            do_sample=True,      # Need sampling for temperature > 0
            pad_token_id=llm_tokenizer.eos_token_id # Use EOS token for padding
        )

        logger.info(f"Running LLM inference for packet {features.get('packet_number', 'N/A')}...")
        with torch.no_grad(): # Ensure no gradients are calculated during inference
            output = llm_model.generate(**inputs, generation_config=generation_config)

        # Decode the output, skipping special tokens like padding/eos
        raw_output = llm_tokenizer.decode(output[0], skip_special_tokens=True)
        logger.debug(f"Raw LLM Output:\n{raw_output}")

        # Extract JSON from the raw output
        analysis_result = extract_json_output(raw_output, prompt)
        if analysis_result is None: # Handle case where extraction fails completely
             analysis_result = {"is_vulnerable": False, "error": "JSON extraction failed"}


    except Exception as e:
        logger.error(f"Error during LLM inference or processing: {e}", exc_info=True)
        analysis_result = {"is_vulnerable": False, "error": f"LLM inference failed: {e}"}

    # Log the final structured result
    if analysis_result.get("is_vulnerable"):
        logger.info(f"LLM Result: Vulnerability Detected - Type: {analysis_result.get('vulnerability_type', 'N/A')}")
    elif "error" in analysis_result:
         logger.warning(f"LLM Result: Error - {analysis_result['error']}")
    else:
         logger.info(f"LLM Result: No vulnerability detected.")


    return analysis_result