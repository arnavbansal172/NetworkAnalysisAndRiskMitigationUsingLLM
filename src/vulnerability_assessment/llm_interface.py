import logging
import json
import torch
import re
import os
from pathlib import Path
from typing import Dict, Any, Optional
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    GenerationConfig,
    BitsAndBytesConfig,
    logging as hf_logging
)
from peft import PeftModel, PeftConfig # Needed to load LoRA adapter

hf_logging.set_verbosity_error() # Reduce transformers verbosity
logger = logging.getLogger(__name__)

# --- Global Variables ---
# Cached model, tokenizer, and adapter path to avoid reloading
_loaded_model = None
_loaded_tokenizer = None
_loaded_adapter_path = None # Track which adapter is loaded
_llm_device = None

# Default Base Model - can be overridden
DEFAULT_BASE_MODEL = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

# --- Model Loading Function ---
def load_llm_model_with_adapter(
    base_model_name_or_path: str = DEFAULT_BASE_MODEL,
    adapter_path: Optional[str | Path] = None,
    force_reload: bool = False
):
    """
    Loads the base LLM model and optionally applies a PEFT adapter.
    Caches the loaded model and tokenizer to avoid reloading.

    Args:
        base_model_name_or_path: Identifier of the base model.
        adapter_path: Path to the PEFT adapter directory. If None, loads base model only.
        force_reload: If True, forces reloading even if cached.

    Returns:
        True if the model/tokenizer is ready, False otherwise.
    """
    global _loaded_model, _loaded_tokenizer, _loaded_adapter_path, _llm_device

    adapter_path_str = str(adapter_path) if adapter_path else None

    # Check cache only if not forcing reload
    if not force_reload and _loaded_model is not None and _loaded_tokenizer is not None:
        # Check if the correct adapter is already loaded
        if _loaded_adapter_path == adapter_path_str:
            logger.debug(f"Using cached model: {base_model_name_or_path} + Adapter: {adapter_path_str or 'None'}")
            return True
        else:
             logger.info(f"Adapter changed (from '{_loaded_adapter_path}' to '{adapter_path_str}'). Reloading model.")
             # Fall through to reload
    elif force_reload:
         logger.info("Forcing model reload.")

    # --- Clear existing model from memory ---
    if _loaded_model is not None:
        logger.info("Clearing previously loaded model from memory...")
        del _loaded_model
        del _loaded_tokenizer
        _loaded_model = None
        _loaded_tokenizer = None
        _loaded_adapter_path = None
        if torch.cuda.is_available():
            torch.cuda.empty_cache() # Attempt to clear GPU cache

    # --- Load New Model ---
    logger.info(f"Loading base model: {base_model_name_or_path}")
    if adapter_path_str:
        logger.info(f"Attempting to apply adapter from: {adapter_path_str}")

    try:
        # Determine compute dtype and device
        if torch.cuda.is_available():
            _llm_device = torch.device("cuda")
            compute_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
            model_kwargs = {"torch_dtype": compute_dtype, "device_map": "auto"}
            logger.info(f"Using GPU. Loading with dtype: {compute_dtype}.")
        else:
            _llm_device = torch.device("cpu")
            model_kwargs = {} # Use default dtype for CPU
            logger.info("Using CPU.")

        # Load Tokenizer
        tokenizer = AutoTokenizer.from_pretrained(base_model_name_or_path, trust_remote_code=True)
        if tokenizer.pad_token_id is None:
            tokenizer.pad_token_id = tokenizer.eos_token_id
        tokenizer.padding_side = 'right' # Set explicitly for generation

        # Load Base Model
        model = AutoModelForCausalLM.from_pretrained(
            base_model_name_or_path,
            **model_kwargs,
            trust_remote_code=True
        )
        model.config.pad_token_id = tokenizer.pad_token_id # Sync model config

        # Load and Apply PEFT Adapter if path is provided
        if adapter_path_str:
            adapter_path_obj = Path(adapter_path_str)
            if not adapter_path_obj.is_dir():
                 logger.error(f"Adapter directory not found: {adapter_path_str}")
                 return False
            logger.info(f"Loading PEFT adapter from {adapter_path_str}...")
            # Load the adapter onto the base model
            model = PeftModel.from_pretrained(model, adapter_path_str)
            logger.info(f"Successfully loaded adapter '{adapter_path_str}'.")
            # Optional: Merge for potentially faster inference, but increases memory slightly
            # logger.info("Merging adapter weights...")
            # model = model.merge_and_unload()
            # logger.info("Adapter merged and unloaded.")

        model.eval() # Set to evaluation mode

        # Cache the loaded model, tokenizer, and adapter path
        _loaded_model = model
        _loaded_tokenizer = tokenizer
        _loaded_adapter_path = adapter_path_str
        logger.info(f"Model '{base_model_name_or_path}' ({'Adapter: '+adapter_path_str if adapter_path_str else 'Base Only'}) ready on {_llm_device}.")
        return True

    except Exception as e:
        logger.error(f"Failed to load model/adapter: {e}", exc_info=True)
        # Clean up global state on error
        _loaded_model = None
        _loaded_tokenizer = None
        _loaded_adapter_path = None
        _llm_device = None
        return False

# --- Prompt Creation ---
def create_llm_prompt(features: Dict[str, Any]) -> str:
    """Creates the chatML-like prompt for TinyLlama Chat model inference."""
    # Re-create the prompt structure used during fine-tuning, *without* the assistant part.
    # Use f-strings for cleaner formatting
    feature_lines = []
    for key, value in features.items():
        # Skip raw bytes or excessively long values in prompt for brevity/clarity
        if isinstance(value, bytes):
            value_str = f"[bytes, length={len(value)}]"
        elif isinstance(value, str) and len(value) > 150:
             value_str = value[:150] + "..."
        else:
             value_str = str(value)
        if value is not None: # Only include features that have values
             feature_lines.append(f"- {key.replace('_', ' ').title()}: {value_str}")

    feature_text = "\n".join(feature_lines) if feature_lines else "No features extracted."

    system_prompt = "<|system|>\nYou are a network security analyst. Analyze the provided network packet features for potential vulnerabilities. Respond ONLY with a valid JSON object containing your analysis. Ensure the JSON is complete and correctly formatted.</s>"
    user_prompt = f"""<|user|>
Analyze the following network packet features and determine if they indicate a vulnerability. Respond ONLY with a valid JSON object detailing your findings, including vulnerability type, severity, description, and mitigation steps if applicable.

Input Network Data:
{feature_text}</s>"""

    # The prompt ends here, expecting the model to generate the <|assistant|> part
    full_prompt = system_prompt + "\n" + user_prompt + "\n<|assistant|>"
    return full_prompt

# --- JSON Extraction ---
def extract_json_output(raw_output: str, prompt: str) -> Optional[Dict[str, Any]]:
    """Attempts to extract the JSON object from the LLM's raw output."""
    logger.debug("Attempting to extract JSON from raw LLM output.")

    # Isolate the assistant's response part
    assistant_marker = "<|assistant|>"
    if assistant_marker not in raw_output:
        logger.warning("Could not find assistant marker in LLM output.")
        # Try removing prompt if marker missing, might work for simpler cases
        generation_part = raw_output.replace(prompt, "").strip()
    else:
        # Get text after the last assistant marker
        generation_part = raw_output.split(assistant_marker)[-1].strip()

    # Clean potential trailing EOS token if present
    if generation_part.endswith("</s>"):
        generation_part = generation_part[:-len("</s>")].strip()

    logger.debug(f"Isolated generation part (first 500 chars): {generation_part[:500]}")

    # Use regex to find JSON block (handles potential leading/trailing text)
    # Looks for '{' potentially preceded/followed by whitespace/newlines, then .*?, then '}'
    match = re.search(r"^\s*(\{.*?\})\s*$", generation_part, re.DOTALL)

    if match:
        json_string = match.group(1)
        logger.debug(f"Potential JSON string found: {json_string}")
        try:
            parsed_json = json.loads(json_string)

            # Basic validation
            is_vuln = parsed_json.get("is_vulnerable")
            if is_vuln is True:
                if not all(k in parsed_json for k in ["vulnerability_type", "severity", "description", "mitigation_steps"]):
                    logger.warning(f"Parsed JSON marked vulnerable but missing required keys: {parsed_json}")
                    return {"is_vulnerable": False, "error": "Incomplete JSON structure from LLM"}
                # Ensure mitigation_steps is a list
                if not isinstance(parsed_json.get("mitigation_steps"), list):
                     logger.warning(f"Mitigation steps is not a list: {parsed_json.get('mitigation_steps')}")
                     # Attempt to fix if it's a string, otherwise fail validation
                     if isinstance(parsed_json.get("mitigation_steps"), str):
                          parsed_json["mitigation_steps"] = [parsed_json["mitigation_steps"]]
                     else:
                          return {"is_vulnerable": False, "error": "Invalid mitigation_steps format"}

            elif is_vuln is False:
                # Allow only 'is_vulnerable' key when false
                if len(parsed_json) > 1:
                    logger.warning(f"Parsed JSON marked not vulnerable but has extra keys: {parsed_json}")
                    return {"is_vulnerable": False} # Return clean version
            else:
                logger.warning(f"Parsed JSON missing or invalid 'is_vulnerable' key: {parsed_json}")
                return {"is_vulnerable": False, "error": "Invalid 'is_vulnerable' key in JSON"}

            logger.debug("JSON parsed and validated successfully.")
            return parsed_json
        except json.JSONDecodeError as e:
            logger.error(f"Failed to decode JSON: {e}\nString was: {json_string}", exc_info=False)
            return {"is_vulnerable": False, "error": f"JSON parsing failed: {e}"}
    else:
        logger.error(f"Could not extract valid JSON object from generation: {generation_part}")
        return {"is_vulnerable": False, "error": "No valid JSON object found in LLM response"}

# --- LLM Query Function ---
def query_llm(features: Dict[str, Any], adapter_path: Optional[str | Path] = None) -> Dict[str, Any]:
    """
    Queries the loaded LLM (base + adapter) with packet features.

    Args:
        features: Dictionary of extracted packet features.
        adapter_path: Path to the PEFT adapter directory. If None, uses base model.

    Returns:
        A dictionary containing the LLM's analysis (e.g., {"is_vulnerable": ...})
        or an error structure (e.g., {"is_vulnerable": False, "error": ...}).
    """
    global _loaded_model, _loaded_tokenizer, _llm_device

    adapter_path_str = str(adapter_path) if adapter_path else None

    # Ensure model is loaded with the correct adapter
    if not load_llm_model_with_adapter(adapter_path=adapter_path_str):
         logger.error("LLM model/adapter is not available.")
         return {"is_vulnerable": False, "error": "LLM model/adapter failed to load"}

    # --- Prepare Prompt and Tokenize ---
    prompt = create_llm_prompt(features)
    # logger.debug(f"LLM Prompt:\n{prompt}") # Verbose

    try:
        logger.debug("Tokenizing prompt...")
        # Important: Do NOT add special tokens again, prompt format includes them
        inputs = _loaded_tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1024, add_special_tokens=False).to(_llm_device)
    except Exception as e:
         logger.error(f"Error tokenizing prompt: {e}", exc_info=True)
         return {"is_vulnerable": False, "error": "Tokenization failed"}

    # --- Configure Generation ---
    # Use low temperature for consistent JSON output, adjust max_new_tokens as needed
    generation_config = GenerationConfig(
        max_new_tokens=350,  # Enough for typical JSON output + some buffer
        temperature=0.1,     # Low temp for deterministic JSON
        do_sample=True,      # Required if temp > 0 or top_p < 1
        pad_token_id=_loaded_tokenizer.eos_token_id, # Use EOS for padding
        eos_token_id=_loaded_tokenizer.eos_token_id # Stop generation at EOS
    )

    # --- Run Inference ---
    analysis_result = {"is_vulnerable": False, "error": "Inference did not produce valid output"} # Default
    logger.info(f"Running LLM inference for packet {features.get('packet_number', 'N/A')}...")
    try:
        with torch.no_grad(): # Disable gradient calculations for inference
            output_ids = _loaded_model.generate(**inputs, generation_config=generation_config)

        # Decode only the generated part (excluding prompt tokens)
        # input_length = inputs['input_ids'].shape[1]
        # generated_ids = output_ids[0, input_length:] # Get only generated token IDs
        # raw_output = _loaded_tokenizer.decode(generated_ids, skip_special_tokens=True) # Skip special tokens in generated part

        # Alternative: Decode full output and extract JSON later (simpler, relies on robust extraction)
        full_output = _loaded_tokenizer.decode(output_ids[0], skip_special_tokens=False) # Keep special tokens for extraction logic
        # logger.debug(f"Raw LLM Output (full):\n{full_output}")

        # --- Extract JSON ---
        parsed_json = extract_json_output(full_output, prompt)
        if parsed_json is not None:
            analysis_result = parsed_json # Contains validated JSON or error structure
        # else analysis_result remains default error state

    except Exception as e:
        logger.error(f"Error during LLM inference or output processing: {e}", exc_info=True)
        analysis_result = {"is_vulnerable": False, "error": f"LLM inference/processing failed: {str(e)}"}

    # --- Log & Return Result ---
    if analysis_result.get("is_vulnerable"):
        logger.info(f"LLM Result: Vulnerability Detected - Type: {analysis_result.get('vulnerability_type', 'N/A')}")
    elif "error" in analysis_result:
         logger.warning(f"LLM Result: Error - {analysis_result['error']}")
    else:
         logger.info(f"LLM Result: No vulnerability detected.")

    return analysis_result