import logging
import json
import sys
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional, Any

# Import necessary feature extractor and interface modules
# Assuming feature_extractor.py exists and has extract_features_from_packet
from . import feature_extractor
# Import the updated interface module
from . import llm_interface
# Import the rule engine
from .rules import RuleEngine, SEVERITY_LEVELS # Import severity levels for potential sorting

# Import Scapy safely
try:
    from scapy.all import PcapReader, sniff, Packet, Scapy_Exception
    # Configure scapy to be less verbose unless DEBUG logging is on
    scapy_log_level = logging.ERROR if logging.getLogger().level > logging.DEBUG else logging.WARNING
    logging.getLogger("scapy.runtime").setLevel(scapy_log_level)
    logging.getLogger("scapy.loading").setLevel(scapy_log_level)
except ImportError:
    sys.exit("Fatal Error: Scapy is not installed. Please run 'pip install scapy'.")
except Exception as e:
     sys.exit(f"Fatal Error: Failed to initialize Scapy: {e}")


logger = logging.getLogger(__name__)

# Global counter for live packets (reset in assess_live)
live_packet_count = 0

# --- Core Packet Processing Logic ---
def _process_single_packet(
    packet: Packet,
    packet_number: int,
    source_id: str,
    rule_engine: RuleEngine,
    llm_adapter_path: Optional[str | Path],
    analyze_with_llm: bool
) -> Optional[Dict[str, Any]]:
    """
    Processes a single packet: extracts features, checks rules, optionally queries LLM.

    Returns: A finding dictionary if vulnerable, else None.
    """
    finding = None
    try:
        # 1. Extract Features
        # Ensure feature_extractor handles potential errors gracefully
        features = feature_extractor.extract_features_from_packet(packet, packet_number, source_id)
        if not features:
            return None # Skip packets with no relevant features (e.g., non-IP)

        # 2. Check Rules First (Efficiency)
        rule_finding = rule_engine.check_packet(features)
        if rule_finding:
            # Use the finding generated by the rule itself
            finding = format_report(features, rule_finding, is_llm_finding=False)
            logger.info(f"Packet {packet_number}: Matched rule '{rule_finding.get('rule_id')}'.")
            return finding

        # 3. Query LLM (if enabled and no rule matched)
        if analyze_with_llm:
            logger.debug(f"Packet {packet_number}: No rule match. Querying LLM...")
            # query_llm returns either the analysis dict or an error dict
            llm_analysis = llm_interface.query_llm(features, adapter_path=llm_adapter_path)

            if llm_analysis.get("is_vulnerable"):
                finding = format_report(features, llm_analysis, is_llm_finding=True)
                # Log LLM finding details
                log_level = logging.WARNING if llm_analysis.get("severity") in [rule_engine.SEVERITY_CRITICAL, rule_engine.SEVERITY_HIGH] else logging.INFO
                logger.log(log_level, f"Packet {packet_number}: LLM detected vulnerability - Type: '{llm_analysis.get('vulnerability_type', 'N/A')}', Severity: '{llm_analysis.get('severity', 'N/A')}'")
            elif "error" in llm_analysis:
                 logger.error(f"Packet {packet_number}: LLM assessment error - {llm_analysis['error']}")
            # else: No LLM finding and no error, log as debug if needed

    except Scapy_Exception as se:
         # Log errors related to packet parsing within Scapy itself
         logger.error(f"Packet {packet_number}: Scapy error processing packet structure: {se}")
    except Exception as e:
        # Catch any other unexpected errors during feature extraction or processing
        logger.error(f"Packet {packet_number}: Unexpected error during processing: {e}", exc_info=True) # Include traceback

    return finding


# --- PCAP Analysis (Iterative) ---
def assess_pcap(
    pcap_file_path: Path,
    rule_engine: RuleEngine,
    model_adapter_path: Optional[str | Path] = None,
    analyze_with_llm: bool = True,
    progress_interval: int = 1000 # Log progress every N packets
) -> List[Dict[str, Any]]:
    """
    Analyzes packets iteratively from a PCAP file using PcapReader.

    Args:
        pcap_file_path: Path object for the PCAP file.
        rule_engine: An initialized RuleEngine instance.
        model_adapter_path: Path to the fine-tuned LLM adapter directory.
        analyze_with_llm: If True, query LLM if no rule matches.
        progress_interval: Log progress every N packets.

    Returns:
        A list of finding dictionaries.
    """
    adapter_path_str = str(model_adapter_path) if model_adapter_path else None # For logging
    logger.info(f"Starting PCAP analysis: {pcap_file_path}")
    logger.info(f"Settings - Rules: Enabled, LLM: {'Enabled' if analyze_with_llm else 'Disabled'}, Adapter: {adapter_path_str or 'None (Base Model)'}")

    all_findings: List[Dict[str, Any]] = []
    packet_count = 0
    processed_with_features = 0
    error_count = 0

    # --- Pre-load LLM (Optional: speeds up first query if LLM is used) ---
    if analyze_with_llm:
         logger.info("Pre-loading LLM model/adapter (if needed)...")
         if not llm_interface.load_llm_model_with_adapter(adapter_path=model_adapter_path):
              logger.error("Failed to load LLM model/adapter. LLM analysis will be skipped.")
              analyze_with_llm = False # Disable LLM if loading failed
         else:
              logger.info("LLM model/adapter ready.")


    try:
        # Use PcapReader for memory efficiency
        with PcapReader(str(pcap_file_path)) as pcap_reader:
            for packet in pcap_reader:
                packet_count += 1
                if packet is None:
                    logger.warning(f"Packet {packet_count}: Read None from PCAP reader (potentially corrupted file?).")
                    error_count += 1
                    continue

                try:
                    finding = _process_single_packet(
                        packet,
                        packet_count,
                        str(pcap_file_path.name), # Source ID is filename
                        rule_engine,
                        model_adapter_path,
                        analyze_with_llm
                    )
                    if finding:
                        all_findings.append(finding)
                    processed_with_features += 1 # Count packets processed by _process_single_packet

                except Exception as e:
                     # Catch errors specifically from _process_single_packet if needed, though it handles internal ones
                     logger.error(f"Critical error processing packet {packet_count}: {e}", exc_info=True)
                     error_count += 1


                # Log progress periodically
                if packet_count % progress_interval == 0:
                    logger.info(f"Processed {packet_count} packets... Findings so far: {len(all_findings)}")

    except FileNotFoundError:
        logger.error(f"PCAP file not found: {pcap_file_path}")
        raise # Re-raise for CLI
    except Scapy_Exception as se:
        # Error opening or reading the file itself
        logger.error(f"Scapy error reading PCAP file {pcap_file_path}: {se}")
        # Depending on the error, might want to raise or just return partial results
        raise Scapy_Exception(f"Failed to read PCAP {pcap_file_path}: {se}") from se
    except Exception as e:
        logger.critical(f"Unexpected critical error during PCAP processing loop: {e}", exc_info=True)
        # Optionally raise, or return partial results
        # raise e

    # --- Final Summary Log ---
    logger.info(f"Finished PCAP analysis of {pcap_file_path}.")
    logger.info(f"Summary - Total packets read: {packet_count}, "
                f"Packets processed (with features): {processed_with_features}, "
                f"Processing errors: {error_count}, "
                f"Total findings generated: {len(all_findings)}")
    return all_findings


# --- Live Analysis Callback Creator ---
def process_live_packet_callback(
    rule_engine: RuleEngine,
    model_adapter_path: Optional[str | Path],
    log_file: Optional[Path],
    analyze_with_llm: bool
):
    """Creates a closure for the Scapy sniff callback."""
    global live_packet_count # Use the global counter defined outside
    processed_count_session = 0 # Counter local to this specific sniff session

    def _callback(packet: Packet):
        nonlocal processed_count_session
        global live_packet_count
        live_packet_count += 1 # Increment global counter
        processed_count_session += 1 # Increment session counter

        try:
            finding = _process_single_packet(
                packet,
                live_packet_count, # Use global count for report consistency
                "live_capture", # Source ID
                rule_engine,
                model_adapter_path,
                analyze_with_llm
            )

            if finding:
                # Log finding to file (JSON Lines) or console (JSON object)
                try:
                    # Use compact JSON for logs, no indent
                    finding_json = json.dumps(finding)
                    if log_file:
                        with log_file.open('a', encoding='utf-8') as f:
                            f.write(finding_json + '\n')
                    else:
                        # Print directly to stdout, ensuring flush for immediate visibility
                        print(finding_json, flush=True)
                except (IOError, TypeError) as log_err:
                    logger.error(f"Error logging finding to {log_file or 'console'}: {log_err}")

            # Log progress less frequently for live capture
            if processed_count_session % 100 == 0:
                 logger.debug(f"Processed {processed_count_session} live packets in this session...")

        except Exception as e:
             # Catch errors within the callback to prevent sniff from stopping
             logger.error(f"Error processing live packet #{live_packet_count}: {e}", exc_info=True)

    return _callback


# --- Live Analysis Orchestration ---
def assess_live(
    interface: str,
    rule_engine: RuleEngine,
    packet_count: int = 0,
    log_file: Optional[Path] = None,
    model_adapter_path: Optional[str | Path] = None,
    analyze_with_llm: bool = True
):
    """
    Analyzes live network traffic using rules and optionally LLM.

    Args:
        interface: Network interface name.
        rule_engine: An initialized RuleEngine instance.
        packet_count: Number of packets to capture (0 for continuous).
        log_file: Path object to stream JSON findings (one per line). If None, print to console.
        model_adapter_path: Path to the fine-tuned LLM adapter directory.
        analyze_with_llm: If True, query LLM if no rule matches.
    """
    global live_packet_count
    live_packet_count = 0 # Reset global counter for each new live session

    adapter_path_str = str(model_adapter_path) if model_adapter_path else None # For logging
    logger.info(f"Starting live capture on interface '{interface}'. Packet Limit: {'Infinite' if packet_count == 0 else packet_count}")
    logger.info(f"Settings - Rules: Enabled, LLM: {'Enabled' if analyze_with_llm else 'Disabled'}, Adapter: {adapter_path_str or 'None (Base Model)'}")
    if log_file:
        # Ensure log directory exists
        try:
             log_file.parent.mkdir(parents=True, exist_ok=True)
             logger.info(f"Logging findings (JSON Lines) to: {log_file}")
        except OSError as e:
             logger.error(f"Failed to create directory for log file {log_file}: {e}. Logging to console instead.")
             log_file = None # Fallback to console
    else:
        logger.info("Printing findings (JSON objects) to console.")


    # --- Pre-load LLM (Optional but recommended for live to avoid delay on first finding) ---
    if analyze_with_llm:
         logger.info("Pre-loading LLM model/adapter (if needed) for live analysis...")
         if not llm_interface.load_llm_model_with_adapter(adapter_path=model_adapter_path):
              logger.error("Failed to load LLM model/adapter. LLM analysis will be disabled for this session.")
              analyze_with_llm = False # Disable LLM if loading failed
         else:
              logger.info("LLM model/adapter ready for live analysis.")

    # Create the callback handler
    packet_handler = process_live_packet_callback(
        rule_engine=rule_engine,
        model_adapter_path=model_adapter_path,
        log_file=log_file,
        analyze_with_llm=analyze_with_llm
    )

    try:
        # Start sniffing
        # Use store=False to avoid buffering packets in memory
        # Consider adding filters (bpf_filter='tcp port 80') later if needed
        sniff(iface=interface, prn=packet_handler, count=packet_count, store=False, stop_filter=None)
        # Log completion only if count > 0, otherwise it stops on Ctrl+C
        if packet_count > 0:
             logger.info(f"Sniff finished after capturing {live_packet_count} packets (target: {packet_count}).")

    except PermissionError as pe:
        # Specific error for permissions
        logger.critical(f"Permission denied for capturing on interface '{interface}'. Try running with sudo or administrator privileges.")
        # Re-raise or handle for CLI feedback
        raise PermissionError(f"Capture permission denied on '{interface}'. Run as root/admin.") from pe
    except OSError as oe:
         # Errors like "No such device"
         logger.critical(f"OS error during capture setup on interface '{interface}': {oe}")
         raise OSError(f"Failed to capture on '{interface}': {oe}. Check if interface exists and is up.") from oe
    except Scapy_Exception as se:
         # Errors from Scapy during sniffing initialization or runtime
         logger.critical(f"Scapy error during live capture on '{interface}': {se}")
         raise Scapy_Exception(f"Scapy capture error on '{interface}': {se}") from se
    except KeyboardInterrupt:
         logger.info("Live capture interrupted by user (Ctrl+C).")
         # Graceful exit on Ctrl+C
    except Exception as e:
        # Catch any other unexpected exceptions during sniff
        logger.critical(f"An unexpected error occurred during live capture: {e}", exc_info=True)
        # Re-raise to indicate failure
        raise RuntimeError(f"Unexpected live capture error: {e}") from e
    finally:
         logger.info("Live analysis session concluded.")


# --- Reporting Functions ---
def format_report(features: Dict[str, Any], analysis: Dict[str, Any], is_llm_finding: bool) -> Dict[str, Any]:
    """Combines features and analysis into a standardized report dictionary."""
    finding_source = "LLM" if is_llm_finding else analysis.get("rule_id", "Rule")
    # Use UTC time for consistency
    report_timestamp = datetime.utcnow().strftime("%Y%m%dT%H%M%S.%fZ")
    packet_timestamp_iso = features.get("timestamp") # Assume feature extractor provides ISO format

    # Ensure mitigation steps is a list of strings
    mitigation_steps = analysis.get("mitigation_steps", [])
    if isinstance(mitigation_steps, str):
         mitigation_steps = [mitigation_steps]
    elif not isinstance(mitigation_steps, list):
         mitigation_steps = ["Mitigation data is not a list."]

    mitigation_steps_str = [str(step) for step in mitigation_steps] # Convert all items to string just in case


    report = {
        "report_id": f"VA-{finding_source.upper()}-{report_timestamp}-{features.get('packet_number', 'NA')}",
        "finding_source": finding_source,
        "detection_timestamp": datetime.utcnow().isoformat() + "Z", # Timestamp when finding was generated
        "packet_timestamp": packet_timestamp_iso,
        "source_ip": features.get("src_ip"),
        "source_port": features.get("src_port"),
        "destination_ip": features.get("dst_ip"),
        "destination_port": features.get("dst_port"),
        "protocol": features.get("protocol"),
        "vulnerability_type": analysis.get("vulnerability_type", "Unknown"),
        "description": analysis.get("description", "No description provided."),
        "severity": analysis.get("severity", "Unknown"),
        "evidence": {
            "pcap_source": features.get("source"), # e.g., filename or "live_capture"
            "packet_number": features.get("packet_number"),
            "packet_length": features.get("length"),
            # Include other relevant L3/L4 details extracted
            "ip_version": features.get("ip_version"),
            "src_mac": features.get("src_mac"),
            "dst_mac": features.get("dst_mac"),
            "tcp_flags": features.get("tcp_flags"),
            "icmp_type": features.get("icmp_type"),
            "icmp_code": features.get("icmp_code"),
            # Payload snippet is useful but potentially large/sensitive
            "payload_snippet": features.get("payload_snippet"), # Ensure this is handled safely (limited length)
        },
        "mitigation_steps": mitigation_steps_str
    }
    # Remove keys with None values from evidence for cleaner output
    report["evidence"] = {k: v for k, v in report["evidence"].items() if v is not None}
    return report

def save_report(results: List[Dict[str, Any]], output_path: Path):
    """Saves the list of findings to a JSON file (pretty-printed)."""
    if not output_path:
        logger.debug("Output path is None, skipping report saving.")
        return

    logger.info(f"Saving report with {len(results)} findings to {output_path}...")
    try:
        # Ensure parent directory exists
        output_path.parent.mkdir(parents=True, exist_ok=True)
        with output_path.open('w', encoding='utf-8') as f:
            json.dump(results, f, indent=2) # Use indent for readability
        logger.info(f"Report successfully saved.")
    except IOError as e:
        logger.error(f"Failed to write report to {output_path}: {e}")
        # Optionally raise or inform user via CLI
        raise IOError(f"Could not write report to {output_path}: {e}") from e
    except TypeError as e:
         logger.error(f"Failed to serialize report data to JSON: {e}. Check data types.")
         raise TypeError(f"Could not serialize report data: {e}") from e

def print_summary(results: List[Dict[str, Any]], max_items: int = 20):
    """Prints a concise summary of findings to the console."""
    if not results:
        print("\n[INFO] No potential vulnerabilities identified in this run.")
        return

    print("\n--- Vulnerability Assessment Summary ---")
    total_findings = len(results)
    rule_findings = sum(1 for r in results if r.get("finding_source", "").lower() != "llm")
    llm_findings = total_findings - rule_findings

    print(f"Total Findings: {total_findings} (Rules: {rule_findings}, LLM: {llm_findings})")

    # Count findings by severity
    severity_counts = {}
    for report in results:
         sev = report.get("severity", "Unknown")
         severity_counts[sev] = severity_counts.get(sev, 0) + 1
    print("Findings by Severity:")
    # Sort severities for display (using defined levels)
    sorted_severities = sorted(severity_counts.keys(), key=lambda x: SEVERITY_LEVELS.get(x, 0), reverse=True)
    for sev in sorted_severities:
        print(f"  - {sev}: {severity_counts[sev]}")

    print("\n--- Top Findings (Max {}): ---".format(max_items))
    # Sort by severity for display priority
    sorted_results = sorted(results, key=lambda x: SEVERITY_LEVELS.get(x.get("severity", ""), 0), reverse=True)

    for idx, report in enumerate(sorted_results[:max_items]):
        print(f"\nFinding {idx+1}/{total_findings}:")
        print(f"  Source:   {report.get('finding_source', 'N/A')}")
        print(f"  Severity: {report.get('severity', 'N/A')}")
        print(f"  Type:     {report.get('vulnerability_type', 'N/A')}")
        print(f"  Network:  {report.get('source_ip', '?')}:{report.get('source_port','*')} -> {report.get('destination_ip', '?')}:{report.get('destination_port', '*')}")
        desc = report.get('description', 'N/A')
        print(f"  Desc:     {desc[:120] + '...' if len(desc) > 120 else desc}") # Truncate
        mitigation = report.get('mitigation_steps', ['N/A'])
        first_step = mitigation[0] if mitigation else 'N/A'
        print(f"  Mitigate: {first_step[:100] + '...' if len(first_step) > 100 else first_step}")

    if total_findings > max_items:
        print(f"\n... (summary truncated, showing first {max_items} findings based on severity)")
    print("--------------------------------------")

# --- Placeholder for future report listing from DB ---
def list_reports_func(severity: Optional[str] = None, limit: int = 20):
    """Placeholder function to list reports from storage."""
    logger.info(f"Attempting to list reports (Severity: {severity}, Limit: {limit})... (Not implemented)")
    print("[NOTE] Report listing requires database integration (not implemented).")
    pass
