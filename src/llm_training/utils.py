# src/llm_training/utils.py
import logging
import json
from transformers import PreTrainedTokenizer

logger = logging.getLogger(__name__)

def load_data_from_jsonl(file_path: str) -> list[dict]:
    """Loads data from a JSON Lines file."""
    data = []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    data.append(json.loads(line))
                except json.JSONDecodeError:
                    logger.warning(f"Skipping invalid JSON line in {file_path}: {line.strip()}")
        logger.info(f"Loaded {len(data)} examples from {file_path}")
        return data
    except FileNotFoundError:
        logger.error(f"Data file not found: {file_path}")
        raise
    except Exception as e:
        logger.error(f"Error reading data file {file_path}: {e}")
        raise

def format_prompt(example: dict) -> str:
    """Formats the prompt string according to Llama V2's instruction format."""
    # Assumes dataset has 'instruction', 'input', 'output' keys
    instruction = example.get("instruction", "")
    input_context = example.get("input", "") # Input here is the packet features text
    output = example.get("output", "") # Target completion (JSON string)

    if input_context:
        prompt = f"""[INST] <<SYS>>\nYou are a helpful network security analysis assistant. Analyze the provided network data and respond ONLY with the requested JSON output.\n<</SYS>>\n\n{instruction}\n\nInput Network Data:\n{input_context} [/INST]\n{output}"""
    else:
        # Handle cases maybe without separate input context? (Less likely here)
        prompt = f"""[INST] <<SYS>>\nYou are a helpful network security analysis assistant. Analyze the provided network data and respond ONLY with the requested JSON output.\n<</SYS>>\n\n{instruction} [/INST]\n{output}"""

    return prompt


def preprocess_data_for_sft(batch: dict, tokenizer: PreTrainedTokenizer, max_length: int = 1024) -> dict:
    """
    Prepares a batch of data for supervised fine-tuning (SFT)
    by formatting and tokenizing. Uses the 'text' field generated by SFTTrainer.
    """
    # SFTTrainer typically prepares a 'text' field by formatting prompt+completion
    # based on the `formatting_func` provided to it (like format_prompt above).
    # We just need to tokenize this combined text.
    tokenized_batch = tokenizer(
        batch["text"],
        max_length=max_length,
        truncation=True,
        padding="max_length", # Pad to max_length
        # return_tensors="pt", # Trainer handles tensor conversion
    )

    # SFTTrainer usually handles labels automatically by masking prompt tokens.
    # If not using SFTTrainer, you would manually create labels here (e.g., copy input_ids
    # and set prompt token positions to -100 ignore index).

    return tokenized_batch